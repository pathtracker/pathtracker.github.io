<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>PathTracker - Tracking Without Re-recognition in Humans and Machines</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Tracking Without Re-recognition in Humans and Machines" />
	<meta property="og:description" content="Tracking Without Re-recognition in Humans and Machines with PathTracker and TrackingNet" />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">Tracking Without Re-recognition in Humans and Machines</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=100px>
						<center>
							<span style="font-size:24px"><a href="mailto:drew_linsley@brown.edu">Drew Linsley <sup>*</sup>,</a></span>
							<span style="font-size:24px"><a href="mailto:malik.gi@northeastern.edu">Girik Malik <sup>*</sup>,</a></span>
							<br/>
							<span style="font-size:24px"><a href="mailto:junkyung@deepmind.com">Junkyung Kim,</a></span>
							<span style="font-size:24px"><a href="mailto:lakshmi_govindarajan@brown.edu">Lakshmi N Govindarajan,</a></span>
							<br/>
							<span style="font-size:24px"><a href="mailto:e.mingolla@northeastern.edu">Ennio Mingolla <sup>^</sup>,</a></span>
							<span style="font-size:24px"><a href="mailto:thomas_serre@brown.edu">Thomas Serre <sup>^</sup></a></span>
						</center>
					</td>
				</tr>
			</table>
			<table align=center width=250px>
				<tr>
					<!-- <td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href="" target="_blank">[Paper]</a></span>
						</center>
					</td> -->
					<td align=center width=120px>
						<center>
							<br/>
							<span style="font-size:20px"><a href="https://arxiv.org/abs/2105.13351" target="_blank">[Paper]</a></span>
							<span style="font-size:20px"><a href="https://github.com/pathtracker-code/" target="_blank">[Code]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	
	<!-- <hr> -->
	<center>
		<table align=center width=850px>
			<tr>
				<td width=260px>
					<center>
						<img class="round" style="width:500px" src="./resources/figures/shibuya_crossing.gif"/>
						<!-- <p>Imagine trying to track the second bird from the left (at the start of the video). <br/> Now think about how would you track the same bird if there were many more of these in the scene. <br/> What strategy would you employ?</p> -->
					</center>
				</td>
			</tr>
		</table>
	</center>
	<br/>
	<table align=center width=850px>
		<!-- <center><h1>Abstract</h1></center> -->
		<tr>
			<td>
				Imagine trying to track one particular pedestrian, fly, or bird in a crowd of many. We introduce <i><b>PathTracker</b></i>, a synthetic visual challenge inspired by classical cognitive psychology experiments for object tracking, which asks human observers and machines to track a target object in the midst of identical-looking but irrelevant objects. The most successful deep neural networks for visual tracking are optimized for recognizing objects in static images, and "tracking" subsequently requires "re-recognizing" an object in temporally successive or disjoint frames. While humans effortlessly learn <i>PathTracker</i> and generalize to systematic variations in task design, state-of-the-art video analysis architectures struggle to match human performance. To solve <i>PathTracker</i>, and make progress towards the greater goal of improving object tracking in machines, we identify and model circuit mechanisms in biological brains that have been implicated in tracking. When instantiated in a deep neural network, our circuit model learns to solve <i>PathTracker</i> by adopting a multi-object tracking strategy in cases of collisions or near-misses between the target object and irrelevant objects. The circuit model learns to use this strategy despite no explicit constraints to do so, and explains a significant proportion of human decision-making on the challenge. We demonstrate that adding our circuit model to a state-of-the-art transformer-based architecture for object tracking builds tolerance to visual nuisances that affect object appearance, such as variations in lightness and occlusion, and ultimately achieves state-of-the-art performance on the large-scale TrackingNet object tracking challenge.
				
			</td>
		</tr>
	</table>
	<br>
	
	
	<hr>
	<center><h1>PathTracker Challenge</h1></center>
	<p align="center">
		<!-- <iframe width="660" height="395" src="https://www.youtube.com/embed/dQw4w9WgXcQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen align="center"></iframe> -->
		<img class="round" style="width:256px" src="./resources/figures/PT_64_14_pos.gif"/>
		<img class="round" style="width:256px" src="./resources/figures/PT_64_14_neg.gif"/>
	</p>
	<p align="center">Try to track the dot leaving the red marker in the above videos. Does it go into the blue marker? <br/> How did you track that target dot? Did you "index" and track it in some way across the video? <br/><br/>
	Most state-of-the-art neural networks in the field of tracking are unable to compete on this task which is seemingly easy for humans. These networks generally rely on the appearance of the object, and try to identify it in every frame of the video. They struggle in solving a task like the PathTracker challenge, because recognizing the target dot at every frame will return them the target along with indistinguishable identical-looking distractors.
	</p>

	<hr>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px">Index and Track (InT)</span>
				<p><img class="round" style="width:600px" src="./resources/figures/model.png"/></p>
				<p>We present Index and Track circuit. Please refer to the paper for more details.</p>
			</center>
		</tr>
	</table>

	<hr>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px">Results</span>
				<p><img class="round" style="width:1000px" src="./resources/figures/pt_results.png"/></p>
				<p>We trained and tested a wide variety of models on our PathTracker challenge, ranging from convolutional to transformer-based to reccurent. We found that our Index and Track circuit performed significantly better than any other model, rivaling human accuracy. 
				<br/><br/>
				Below are a few more videos explaining the strategy of our model on PathTracker challenge.
				</p>
				<p>
				<img class="round" style="width:600px" src="./resources/figures/PT_results/0.gif"/>
				<img class="round" style="width:600px" src="./resources/figures/PT_results/1.gif"/>
				<img class="round" style="width:600px" src="./resources/figures/PT_results/3.gif"/>
				<img class="round" style="width:600px" src="./resources/figures/PT_results/4.gif"/> </p>

			</center>
		</tr>
	</table>

	<hr>

	<table align=center width=800px>
		<br>
		<tr>
			<center>
				<span style="font-size:28px">Extension of Index and Track to natural video domain</span>
				<p>We extended our model with TimeSformer network to track target objects in GOT-10K testset. 
				<br/> We achieved new state-of-the-art performance on large-scale TrackingNet object tracking challenge.</p>
				<p>
				<img class="round" style="width:600px" src="./resources/figures/GOT_results/GOT-10k_Test_000006.gif"/>
				<img class="round" style="width:600px" src="./resources/figures/GOT_results/GOT-10k_Test_000015-2.gif"/>
				<img class="round" style="width:600px" src="./resources/figures/GOT_results/GOT-10k_Test_000056.gif"/>
				<img class="round" style="width:600px" src="./resources/figures/GOT_results/GOT-10k_Test_000070.gif"/>
				<img class="round" style="width:600px" src="./resources/figures/GOT_results/GOT-10k_Test_000151.gif"/>
				</p>
			</center>
		</tr>
	</table>

	<hr>

	<center><h1>Code</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<p align="center">We are open sourcing our code and dataset to encourage further advancements in the field.</p>
				<p align="center">
					<a href="https://mega.nz/file/L8A3xS6J#rQB9_FsvddOeOHJWiHSdVPJYF3gI41qaq_Fw9168L0A" target="_blank">[Dataset]</a> <a href="https://github.com/pathtracker-code/" target="_blank">[Code]</a> <a href="http://128.148.254.35/?workerId=NeurIPS&assignmentId=NeurIPS&hitId=NeurIPS" target="_blank">[Human data collection survey]</a>
				</p>
			</tr>
		</center>
	</table>
	
	
	<br>
	<hr>
	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href="https://arxiv.org/abs/2105.13351" target="_blank"><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Linsley D.*, Malik G.*, Kim J., Govindarajan L. N., Mingolla E.^, Serre T.^ <br>
				<b> Tracking Without Re-recognition in Humans and Machines</b><br>
				In Proceedings of the 35th International Conference on Neural Information Processing Systems, 2021<br>
				(hosted on <a href="https://arxiv.org/abs/2105.13351" target="_blank">arXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<!-- <br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table> -->

<br>
</body>
</html>

